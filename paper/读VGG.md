# ABSTRACT
1. 主要贡献是通过很小的（3*3）的卷积核来评估网络深度的影响，结果表明到了16-19层时候会有显著提升。
2. 这帮我们在2014年ImageNet比赛中定位和分类中得了第一和第二。我们还证明了这个方法也可以应用于其他数据集，达到当前最佳水平。
3. 我们发表了表现的最好的2个卷积模型，来促进计算机视觉中对深度视觉表示的进一步研究。

# 1. INTRODUCTION
1. 卷积网络最近在大型的图片和视频识别中很成功，得益于大型的数据集合和高性能的计算系统，比如GPU和大规模分布式集群。
2. 自AlexNet后，计算机视觉领域内也开始使用卷积网络，并尝试改进结构获取更高的精度。比如ILSVRC-2013表现最好的模型，通过在第一层使用了更小的核和步长。还有在整个图和多个尺寸上多次训练和测试网络。
3. 我们着眼于一个方面，网络深度。为此，我们固定了其他参数，只通过加更多的卷积层来增加深度。因为在所有层，都用了3*3的核，所以是可行的。
4. 结果我们发现了结果更好的模型，不止在ILSVRC分类和定位问题中成绩很好，还适用于其他数据集。我们公布了2个表现最好的模型，以便进一步研究。

# 2. CONVNET CONFIGURATIONS
1. 为了客观的测量深度带来的提升。我们所有卷积层的设置的理念都一样。

## 2.1 ARCHITECTURE
1. 对图片唯一的预处理是减去训练集RGB的平均值。3 * 3的核是最小的（可以受到上下左右的影响）。我们还用了1 * 1的核，可以看做是线性变换（后边跟着非线性变换）。padding为same。一共有5个Max-pooling，跟在一些卷积层后边，不是所有卷积层后边都有Max-pooling。不管前边怎么样，最后都跟着3个全连接层。前两个有4096个通道，最后有1000个（对应分类类别数）。再最后是softmax层。所有隐藏层都有relu激活函数。我们没有用LRN标准化。因为发现没有帮助，还提高了内存和计算压力。

## 2.2 CONFIGURATIONS
1. 配置信息见表格，每列是一个模型，可以看到他们只有深度的区别，设计理念都是2.1中提到的。通道数也很少，从一开始的64增长到512。表2中展示了每个模型的可训练参数数目，*尽管深度很深，但比起其他深度小却kenel大或者通道大的来说，我们的参数数目更少*。
[image:854B30FC-FAA8-4312-9139-0748CDDFF3FB-19158-000050C1D3E5B4E7/F578051E-15BF-43F2-BD09-683EFE77EB61.png]
https://dl.dropboxusercontent.com/s/mjhu0cxpdqboukb/屏幕截图%202020-04-18%2016.56.50.png

## 2.3 DISCUSSION
1. 我们的和其他表现好的模型不同的是，在第一层没有用大的感受野。 
2. 2个3 * 3和一个5 * 5的感受野是相同的，3个3 * 3和1个7 * 7的感受野是相同的。
3. 我们通过3个3 * 3替换1个7 * 7得到什么好处：首先我们有3个激活层，可以让决策函数更具有区分性。然后，我们减少了参数数量。（假设输入层和输出层channel都是c，那我们的参数一共 3 * (3 * 3 * c * c) 是27cc， 如果用1个7 * 7的，是49cc，比27多了82%。
4. 1 * 1卷积层是一种不影响感受野的情况下为决策增加非线性的方法，虽然在我们模型中实际上是一个相同空间维度内的线性投影，不过后边的激活函数里有非线性因素。（2014 Lin的Network in Network中也用了这个结构）

# 3. CLASSIFICATION FRAMEWORK
## 3.1 TRAINING
1. 和AlexNet训练过程基本一样，不过没有图形变换后作为输入训练。训练中用了momentum和mini-batch。 batch 256, momentum 0.9，L2 regularization 5*e-4，前2个全连接层有dropout 0.5。lr 0.01， 验证集准确率不提升时以10倍速率衰减。最终，lr降低了3次，37万次迭代后（74 epochs）收敛。虽然参数更多，层数更深，但是需要的epochs反而少，我们推测是因为深度和小核增强了正则化，还可能因为是某些层执行了预初始化。
2. 网络权重的初始化很重要，由于深度网络梯度下降的不稳定性，不好的初始化会阻碍学习。我们采取的办法是，在A模型（很浅），随机初始化并训练。然后训练更深的网络时，前4个卷积层和后3个全连接层直接用A的结果（并且在训练中不降低学习率）。（中间其他层还是随机初始化）。
3. 随机初始化是从0均值，0.01方差的正态分布中取值。初试偏差值为0。
4. 值得注意的是，我们发现可以用Glorot&Benjio(2010)中的随机初始化程序对权重进行初始化，而不需要进行预训练。

```
为了得到固定的224x224的RGB输入图片，我们随机从经过尺寸缩放的训练集图片中进行裁剪（每张图的每次SGD迭代时裁剪一次）。为了进一步对训练集数据进行增强，被裁剪图片将进行随机水平翻转及RGB颜色转换。训练集图片尺寸 我们考虑使用两种方式来设置训练尺寸S。第一种是固定S，针对单尺寸图片的训练。第二种设置S的方式是使用多尺寸图像训练，即每个训练图片的尺寸是[Smin，Smax]之间的随机数（这里使用Smin=256,Smax=512）。由于图像中的对象可能大小不一，所以训练中采用这种方式是有利的。
```
/（这段不太懂）/

# 4. CLASSIFICATION E XPERIMENTS
[image:9FEF2139-C47F-4377-99E1-A8D4BABC6BB0-19158-0000587C9E6C269E/20036829-B78F-4F03-980E-F72CF6E1CCFF.png]
https://dl.dropboxusercontent.com/s/9rp1uvtvaazagcv/屏幕截图%202020-04-18%2019.23.21.png?dl=0
1. C比B多了1 * 1卷积层，说明添加非线性层的确有用。
2. D比C好，说明卷积获取空间上下文信息更有用

# 5. CONCLUSION
略
